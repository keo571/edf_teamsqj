{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The notebook uses beautifulsoup and selenium libraries to scrape raw data from different websites, and each dataset is saved in a json file. \n",
    "\n",
    "### Explanation of Each Websites\n",
    "- Votesmart: VoteSmart is a non-profit organization that provides information on the voting records, issue positions, biographical information, and campaign finances of elected officials and candidates in the United States.\n",
    "- Twitter: Twitter is a social media platform where users can post and interact with short messages called tweets. It is often used by politicians to share their thoughts and communicate with their constituents.\n",
    "- Billtrack: Billtrack is a legislative tracking service that provides up-to-date information on bills and their status in the United States Congress and state legislatures.\n",
    "- Reaproject: Reaproject is a website that focuses on raising awareness about regional economic analysis techniques and methodologies in local and regional communities throughout the US..\n",
    "\n",
    "### Scraping Methodologies\n",
    "- Votesmart: The websites are accessed using selenium and the following steps are taken:\n",
    "    1) Type the politician's name in the search box.\n",
    "    2) Click on the folders for the categories of interest, including votes, ratings, and funding:\n",
    "        2-1) Votes: \n",
    "            2-1-1) Click on the voting category \"environment\" and collect all information.\n",
    "            2-1-2) Click on each bill sequentially and collect information on committee sponsors, sponsors, and co-sponsors. Sponsors or co-sponsors that are currently out-of-office are not collected.\n",
    "        2-2) Ratings: Collect organization and match rate information.\n",
    "        2-3) Funding: Collect top contributors list information.\n",
    "- Twitter: Collect all Twitter posts' text data of the three politicians using the snscrape library.\n",
    "- Billtrack: Collect all information under the three politicians' profiles, including bio, staff, votes and etc.\n",
    "- Reaproject: Collect all employment information divided by each industry in each politician's district."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Votesmart scraping part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "\n",
    "def getPoliticianProfileDriver(name):\n",
    "    '''\n",
    "    typing in politician name on the search box\n",
    "    '''\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')  \n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    s = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=s, options=options)\n",
    "    # driver = webdriver.Chrome(service=s)\n",
    "\n",
    "    driver.get(\"https://justfacts.votesmart.org\")\n",
    "    driver.implicitly_wait(10)\n",
    "    #searching politician\n",
    "    driver.find_element(by=By.XPATH, value='//*[@id=\"ispysearch\"]').send_keys(name)\n",
    "    driver.implicitly_wait(10)\n",
    "    #click on the politician\n",
    "    sleep(0.5)\n",
    "    driver.find_element(by=By.XPATH,value='//*[@id=\"iSpyResultsDropdown\"]')\n",
    "    driver.implicitly_wait(10)\n",
    "    # driver.find_element(by=By.TAG_NAME, value='a').click()\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"iSpy-dropdown-item\")))\n",
    "    driver.find_element(by=By.CLASS_NAME, value='iSpy-dropdown-item').click()\n",
    "    driver.implicitly_wait(10)\n",
    "    return driver\n",
    "\n",
    "def support_button_dismiss(driver):\n",
    "    '''\n",
    "    if support asking button pops up, click to dismiss, otherwise pass.\n",
    "    '''\n",
    "    sleep(1)\n",
    "    support_button = driver.find_element(by=By.XPATH, value='//*[@id=\"helpusPopUpNoThanksQuote\"]')\n",
    "    try:\n",
    "        # Check if the button is displayed on the page\n",
    "        if support_button.is_displayed():\n",
    "            # Click the button\n",
    "            support_button.send_keys(Keys.ENTER)\n",
    "            print(\"Button clicked successfully\")\n",
    "        else:\n",
    "            print(\"Button exists but is not visible on the page\")\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    return driver\n",
    "\n",
    "def getDonorList(name):\n",
    "    print(name)\n",
    "    driver = getPoliticianProfileDriver(name)\n",
    "    sleep(1)\n",
    "    #click on funding\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"fundingFolderClosed\"]/span')))\n",
    "    driver.find_element(by=By.XPATH, value='//*[@id=\"fundingFolderClosed\"]/span').click()\n",
    "    driver.implicitly_wait(10)\n",
    "    driver = support_button_dismiss(driver)\n",
    "    driver.implicitly_wait(10)\n",
    "    html_content = driver.page_source\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # Find the table that contains the top contributors and get the rows\n",
    "    table = soup.find('div', {'id': 'nimspContributors'})\n",
    "    if table is None:\n",
    "        return []\n",
    "    rows = table.find_all('tr')\n",
    "    res_donors=[]\n",
    "    # Print the top contributors and their contribution amounts\n",
    "    for row in rows[1:]:  # skip the header row\n",
    "        cells = row.find_all('td')\n",
    "        name = cells[0].text.strip()\n",
    "        amount = cells[1].text.strip()\n",
    "        tmp = dict()\n",
    "        tmp['name'] = name\n",
    "        tmp['amount'] = amount\n",
    "        res_donors.append(tmp)\n",
    "    return res_donors\n",
    "\n",
    "\n",
    "\n",
    "def getRatingList(name):\n",
    "    print(name)\n",
    "    driver = getPoliticianProfileDriver(name)\n",
    "    #click on funding\n",
    "    driver.find_element(by=By.XPATH, value='/html/body/div[1]/nav[2]/div/div/div/div[3]/div[2]/div/div[2]/div/div[1]/a/span/span').click()\n",
    "    driver.implicitly_wait(10)\n",
    "    driver = support_button_dismiss(driver)\n",
    "    driver.implicitly_wait(10)\n",
    "    html_content = driver.page_source\n",
    "\n",
    "    # Use Beautiful Soup to parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the table that contains the top contributors and get the rows\n",
    "    divs = soup.find_all('div',{'class':'card card-collapse'})\n",
    "    if divs is None:\n",
    "        return []\n",
    "    res_match =[]\n",
    "    for div in divs:\n",
    "        title = div.find('div', {'class':\"col col-sm col-\"})\n",
    "        perc = div.find('div',{'class':'col-md-2 col-sm-2 col-2 evaluations-item-primary text-nowrap'})\n",
    "        tmp = dict()\n",
    "        tmp['name'] = title.text.strip()\n",
    "        tmp['match'] = perc.text.strip()\n",
    "        res_match.append(tmp)\n",
    "    res_match = list({v['name']:v for v in res_match}.values())\n",
    "    #print(res_match)\n",
    "    return res_match\n",
    "\n",
    "\n",
    "def getVotingInfo(name):\n",
    "    driver = getPoliticianProfileDriver(name)\n",
    "    #click on funding\n",
    "    driver.find_element(by=By.XPATH, value='//*[@id=\"votesFolderClosed\"]/span').click()\n",
    "    driver.implicitly_wait(10)\n",
    "    print('vote folder click')\n",
    "    driver = support_button_dismiss(driver)\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.find_element(by=By.ID, value='dropdownMenuButton').send_keys(Keys.ENTER)\n",
    "    driver.implicitly_wait(10)\n",
    "    sleep(1.5)\n",
    "    driver.find_element(by=By.XPATH,value='//*[@id=\"votesCategoriesDropdownMenu\"]/a[20]').send_keys(Keys.ENTER)\n",
    "    driver.implicitly_wait(10)\n",
    "    html_content = driver.page_source\n",
    "\n",
    "    # Use Beautiful Soup to parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the table that contains the top contributors and get the rows\n",
    "    divs = soup.find_all('div',{'class':'card card-collapse'})\n",
    "    res_vote = []\n",
    "    for idx, div in enumerate(divs):\n",
    "        date = div.find('div', {'class':\"col-lg-3 col-md-3 v-congstatus-statusdate\"}).text.strip()\n",
    "        synopsis = div.find('div',{'class':'col col-sm col- text-left'}).text.strip()\n",
    "        vote = div.find('div', {'class':\"col-xl-2 col-lg-3 col-md-3 col-sm-3 col-10 text-left vote-header-collapse-item candidate-votes-congaction-col\"}).text.strip()\n",
    "        val = div.find('div',{'class':'card-body'}).find('tbody').findAll('tr')\n",
    "\n",
    "        outcome = val[1].find('td').text.strip()\n",
    "        bill_no = val[0].find('td').text.strip()\n",
    "        print('before vote title click')\n",
    "        driver = support_button_dismiss(driver)\n",
    "        driver.implicitly_wait(10)\n",
    "        driver.find_elements(by=By.CLASS_NAME, value='vote-title-link')[idx].send_keys(Keys.ENTER)\n",
    "        driver.implicitly_wait(10)\n",
    "        sleep(1)\n",
    "        html_content_bill = driver.page_source\n",
    "        soup_bill = BeautifulSoup(html_content_bill, 'html.parser')\n",
    "        committee_collapse = soup_bill.find_all('ul',{'class':'list-group key-votes-detail-list'})\n",
    "        committee_list = []\n",
    "        committee_title_idx = -1\n",
    "        for idx, collapse in enumerate(committee_collapse):\n",
    "            if collapse.parent.find('h4', {'class':\"title\"}) is None:\n",
    "                continue\n",
    "            if collapse.parent.find('h4', {'class':\"title\"}).text.strip() == 'Committee Sponsors':\n",
    "                committee_title_idx = idx\n",
    "        # if committee_collapse[3].parent.find('h4', {'class':\"title\"}) is not None:\n",
    "        #     if committee_collapse[3].parent.find('h4', {'class':\"title\"}).text.strip() == 'Committee Sponsors':\n",
    "        if committee_title_idx != -1:\n",
    "            committee_list_html = committee_collapse[committee_title_idx].find_all('li',{'class':'list-group-item'})          \n",
    "            for committee_html in committee_list_html:\n",
    "                committee_list.append(committee_html.text.strip())\n",
    "        sponsor_list = []\n",
    "        sponsor_collapse = soup_bill.find('div',{'id':'collapseSponsor'})\n",
    "        if sponsor_collapse is not None: \n",
    "            sponsor_list_html = sponsor_collapse.find_all('li',{'class':'list-group-item'})\n",
    "            for sponsor_html in sponsor_list_html:\n",
    "                sponsor_name_with_space = sponsor_html.text.strip()\n",
    "                #organize space in between\n",
    "                sponsor_name = \" \".join(sponsor_name_with_space.split())\n",
    "                #if out of office, not included in the list\n",
    "                if '(Out Of Office)' not in sponsor_name:\n",
    "                    sponsor_list.append(sponsor_name)\n",
    "        cosponsor_list = []\n",
    "        cosponsor_collapse = soup_bill.find('div',{'id':'collapseCo-sponsor'})\n",
    "        if cosponsor_collapse is not None: \n",
    "            cosponsor_list_html = cosponsor_collapse.find_all('li',{'class':'list-group-item'})\n",
    "            for cosponsor_html in cosponsor_list_html:\n",
    "                cosponsor_name_with_space = cosponsor_html.text.strip()\n",
    "                #organize space in between\n",
    "                cosponsor_name = \" \".join(cosponsor_name_with_space.split())\n",
    "                #if out of office, not included in the list\n",
    "                if '(Out Of Office)' not in cosponsor_name:\n",
    "                    cosponsor_list.append(cosponsor_name)               \n",
    "        driver.back()\n",
    "        tmp = dict()\n",
    "        tmp['Bill No.'] = bill_no\n",
    "        tmp['Date'] = date\n",
    "        tmp['Synopsis'] = synopsis\n",
    "        tmp['Vote'] = vote        \n",
    "        tmp['Outcome'] = outcome\n",
    "        tmp['Committee'] = committee_list\n",
    "        tmp['Sponsor'] = sponsor_list\n",
    "        tmp['Cosponsor'] = cosponsor_list\n",
    "        res_vote.append(tmp)\n",
    "    return res_vote\n",
    "\n",
    "if __name__ == '__main__':  \n",
    "    import json\n",
    "    #first depth collecting\n",
    "    politician_name_list=['Karen Spilka','Ana Rodriguez','Liz Krueger']\n",
    "    #donor\n",
    "    container=dict()\n",
    "    for idx, name in enumerate(politician_name_list):\n",
    "        container[name] = getDonorList(name)\n",
    "\n",
    "    with open(\"raw_data/votesmart_donors.json\", \"w\") as fp:\n",
    "        json.dump(container,fp) \n",
    "\n",
    "    #rating\n",
    "    container=dict()\n",
    "    for idx, name in enumerate(politician_name_list):\n",
    "        container[name] = getRatingList(name)\n",
    "\n",
    "    with open(\"raw_data/votesmart_match.json\", \"w\") as fp:\n",
    "        json.dump(container,fp) \n",
    "\n",
    "    # voting + second depth politicians\n",
    "    container=dict()\n",
    "    second_depth_politician_list = []\n",
    "    for idx, name in enumerate(politician_name_list):\n",
    "        container[name] = getVotingInfo(name)\n",
    "        for bill_info in container[name]:\n",
    "            second_depth_politician_list.extend(bill_info['Sponsor'])\n",
    "            second_depth_politician_list.extend(bill_info['Cosponsor'])\n",
    "    with open(\"raw_data/votesmart_vote.json\", \"w\") as fp:\n",
    "        json.dump(container,fp) \n",
    "\n",
    "    # #saving only unique values\n",
    "    second_depth_politician_list = set(second_depth_politician_list)\n",
    "    second_depth_politician_list=list(second_depth_politician_list)\n",
    "    with open(\"raw_data/second_depth_politician_list.json\", \"w\") as fp:\n",
    "        json.dump(second_depth_politician_list,fp) \n",
    "\n",
    "    # #Second depth collecting\n",
    "    f = open('raw_data/second_depth_politician_list.json')\n",
    "    second_depth_politician_list = json.load(f)\n",
    "\n",
    "    # Second depth donor\n",
    "    container = dict()\n",
    "    for idx, name in enumerate(second_depth_politician_list):\n",
    "        container[name] = getDonorList(name)\n",
    "\n",
    "    with open(\"raw_data/votesmart_donors_second_depth.json\", \"w\") as fp:\n",
    "        json.dump(container,fp) \n",
    "\n",
    "    # Second depth rating\n",
    "    container=dict()\n",
    "    for idx, name in enumerate(second_depth_politician_list):\n",
    "        container[name] = getRatingList(name)\n",
    "\n",
    "    with open(\"raw_data/votesmart_match_second_depth.json\", \"w\") as fp:\n",
    "        json.dump(container,fp) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter scraping part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.system(\"snscrape --jsonl --max-results 100 twitter-search 'from:KarenSpilka'> user-tweets.json\")\n",
    "\n",
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import json\n",
    "politician_twitter_ids=[\"KarenSpilka\",\"SenatorAMR\",\"LizKrueger\"]\n",
    "politician_names = [\"Karen Spilka\", \"Ana Rodriguez\",\"Liz Krueger\"]\n",
    "# Creating list to append tweet data to\n",
    "data_retrieved_for_each_politicians = []\n",
    "ret = {}\n",
    "for id in politician_twitter_ids:\n",
    "    tmp = []\n",
    "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:{}'.format(id), top = True).get_items()):\n",
    "        if i>100:\n",
    "            break\n",
    "        # tmp.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "        tmp.append([tweet.date, tweet.content])\n",
    "    # Creating a dataframe from the tweets list above \n",
    "    tweets_df = pd.DataFrame(tmp, columns=['Datetime',  'Text'])\n",
    "    data_retrieved_for_each_politicians.append(tweets_df.to_json())\n",
    "for idx, data in enumerate(data_retrieved_for_each_politicians):\n",
    "    ret[politician_names[idx]]=json.loads(data) \n",
    "\n",
    "with open(\"raw_data/twitter_posts.json\", \"w\") as fp:\n",
    "    json.dump(ret,fp) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Billtrack scraping part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url_list = ['https://www.billtrack50.com/legislatordetail/16923','https://www.billtrack50.com/legislatordetail/23016','https://www.billtrack50.com/legislatordetail/2226']\n",
    "name_list=['Karen Spilka','Ana Rodriguez','Liz Krueger']\n",
    "res = dict()\n",
    "for idx, url in enumerate(url_list):\n",
    "    tmp_data_container={}\n",
    "    # Send a GET request to the URL and get the HTML content\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    # Use Beautiful Soup to parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the table that contains the top contributors and get the rows\n",
    "    table = soup.find('div', {'id': 'bills'})\n",
    "    if table is not None:\n",
    "        tbody = table.find('tbody')\n",
    "        rows = tbody.find_all('tr')\n",
    "        tmp_data_container['bills'] = []\n",
    "        # Print the top contributors and their contribution amounts\n",
    "        for row in rows:  # skip the header row\n",
    "            cells = row.find_all('td')\n",
    "            bill = cells[0].text.strip()\n",
    "            billname = cells[1].text.strip()\n",
    "            summary = cells[2].text.strip()\n",
    "            progress = cells[3].text.strip()\n",
    "    #        print(f'{name}: {amount}')\n",
    "            tmp = dict()\n",
    "            tmp['bill'] = bill\n",
    "            tmp['billname'] = billname\n",
    "            tmp['summary'] = summary\n",
    "            tmp['progress'] = progress\n",
    "            tmp_data_container['bills'].append(tmp)\n",
    "        \n",
    "    table = soup.find('div', {'id': 'votes'})\n",
    "    if table is not None:\n",
    "        tbody = table.find('tbody')\n",
    "        rows = tbody.find_all('tr')\n",
    "        tmp_data_container['votes'] =[]\n",
    "        # Print the top contributors and their contribution amounts\n",
    "        for row in rows:  # skip the header row\n",
    "            cells = row.find_all('td')\n",
    "            bill = cells[0].text.strip()\n",
    "            billname = cells[1].text.strip()\n",
    "            motion = cells[2].text.strip()\n",
    "            votedate = cells[3].text.strip()\n",
    "            vote = cells[4].text.strip()\n",
    "    #        print(f'{name}: {amount}')\n",
    "            tmp = dict()\n",
    "            tmp['bill'] = bill\n",
    "            tmp['billname'] = billname\n",
    "            tmp['motion'] = motion                                  \n",
    "            tmp['votedate'] = votedate\n",
    "            tmp['vote'] = vote\n",
    "            tmp_data_container['votes'].append(tmp)\n",
    "\n",
    "\n",
    "    table = soup.find('div', {'id': 'committee'})\n",
    "    if table is not None:\n",
    "        tbody = table.find('tbody')\n",
    "        rows = tbody.find_all('tr')\n",
    "        tmp_data_container['committee'] =[]\n",
    "        # Print the top contributors and their contribution amounts\n",
    "        for row in rows:  # skip the header row\n",
    "            cells = row.find_all('td')\n",
    "            role = cells[1].text.strip()\n",
    "            committee = cells[2].text.strip()\n",
    "    #        print(f'{name}: {amount}')\n",
    "            tmp = dict()\n",
    "            tmp['role'] = role\n",
    "            tmp['committee'] = committee\n",
    "            tmp_data_container['committee'].append(tmp)\n",
    "\n",
    "    table = soup.find('div', {'id': 'staff'})\n",
    "    if table is not None:\n",
    "        tbody = table.find('tbody')\n",
    "        rows = tbody.find_all('tr')\n",
    "        tmp_data_container['staff'] = []\n",
    "        # Print the top contributors and their contribution amounts\n",
    "        for row in rows:  # skip the header row\n",
    "            cells = row.find_all('td')\n",
    "            name = cells[0].text.strip()\n",
    "            title = cells[1].text.strip()\n",
    "            role_description = cells[2].text.strip()\n",
    "            location = cells[3].text.strip()\n",
    "            address = cells[4].text.strip()\n",
    "            phone = cells[5].text.strip()\n",
    "            email = cells[6].text.strip()\n",
    "    #        print(f'{name}: {amount}')\n",
    "            tmp = dict()\n",
    "            tmp['name'] = name\n",
    "            tmp['title'] = title\n",
    "            tmp['role description'] = role_description\n",
    "            tmp['location'] = location\n",
    "            tmp['address'] = address\n",
    "            tmp['phone'] = phone\n",
    "            tmp['email'] = email\n",
    "            tmp_data_container['staff'].append(tmp)\n",
    "\n",
    "\n",
    "    table = soup.find('div', {'id': 'bio'})\n",
    "    if table is not None:\n",
    "        biotext = table.text.strip()\n",
    "    #        print(f'{name}: {amount}')\n",
    "        tmp = dict()\n",
    "        tmp['bio'] = biotext\n",
    "        tmp_data_container['bio'] = tmp\n",
    "\n",
    "    res[name_list[idx]] = tmp_data_container\n",
    "\n",
    "\n",
    "\n",
    "with open(\"raw_data/billtrack_info.json\", \"w\") as fp:\n",
    "    json.dump(res,fp) \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reaproject scraping part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url_list = ['https://massachusetts.reaproject.org/analysis/industry-structure/industries_by_region/employment/tools/250017/','https://florida.reaproject.org/analysis/industry-structure/industries_by_region/employment/tools/120087/','https://new-york.reaproject.org/analysis/industry-structure/industries_by_region/employment/tools/360061/']\n",
    "final_res = dict()\n",
    "name_list=['Karen Spilka','Ana Rodriguez','Liz Krueger']\n",
    "\n",
    "##Donor info scraping\n",
    "for idx, url in enumerate(url_list):\n",
    "    # Send a GET request to the URL and get the HTML content\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    # Use Beautiful Soup to parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the table that contains the top contributors and get the rows\n",
    "    page2 = soup.find('div', {'id': 'page_2'})\n",
    "    table = page2.find('div', {'class': 'report_table_data'})\n",
    "    rows = table.find_all('tr')\n",
    "    res=[]\n",
    "    # Print the top contributors and their contribution amounts\n",
    "    for row in rows[2:]:  # skip the header row\n",
    "        cells = row.find_all('td')\n",
    "        industry = cells[0].text.strip()\n",
    "        jobs = cells[2].text.strip()\n",
    "        percent = cells[3].text.strip()\n",
    "        location_quotient = cells[4].text.strip()\n",
    "\n",
    "        tmp = dict()\n",
    "        tmp['industry']=industry\n",
    "        tmp['jobs'] = jobs\n",
    "        tmp['percent'] = percent\n",
    "        tmp['location_quotient'] = location_quotient\n",
    "        res.append(tmp)\n",
    "    #print(res_donors)\n",
    "    final_res[name_list[idx]] = res\n",
    "\n",
    "with open(\"raw_data/industry_employment.json\", \"w\") as fp:\n",
    "    json.dump(final_res,fp) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMX6QXENXLiFMoT7zyqZDMQ",
   "mount_file_id": "16MFEE2-pjElbzOeAMUUPWyqEN1XAPI64",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb5f80828f4edc7fb3676945f8194e7a59df01c454ffb90d9e417412cb0cde25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
